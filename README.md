This paper presents a new method for the colorization of grayscale images by leveraging Generative Adversarial Networks to automatically endow images with realistic colors. Traditional colorization methods are labor-intensive and often achieve low scalability since they rely on the prowess of human beings undertaking the task. GANs seize the opportunity of the generator-discriminator architecture that enables it to be trained in such a way that captures complex color distributions and spatial dependencies in images, thus being the best option for high-quality automatic colorization. Our model will leverage the recent advances in conditional GANs and self-attention mechanisms. Semantic guidance will be integrated to better serve the goal of color accuracy and preserving detail.
Our experimental setup deals with a diverse set of grayscale/color image pairs augmented with appropriate data augmentations to ensure well generalized models are developed. We shall measure performance with quantitative metrics in the form of SSIM, PSNR and Fr√©chet Inception Distance along with qualitative assessment using human studies. Results show the ability of the model to produce aesthetically pleasing, contextually correct colorizations both in terms of realism and consistency over baselines. This paper highlights GANs as powerful scalable colorization tools in many different fields, from historical restoration to medical imaging and digital media, towards support toward SDG 9: Industry, Innovation, and Infrastructure. Future work will continue toward efficiency improvements that may make it viable for real-time applications.
